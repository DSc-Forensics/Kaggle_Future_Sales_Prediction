{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b348b9",
   "metadata": {},
   "source": [
    "Talk about how below datasets can be downloaded from Kaggle competition. How there are several commented statements besides the running code in the notebook, indicating that the commented code was either found to be less efficient or a worse approach than the existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f0d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir(\"C:/Users/PC/Desktop/Programming/Kaggle/1C_Sales\")\n",
    "\n",
    "sales=pd.read_csv(\"Inputs/sales_train.csv\")     \n",
    "item_cat=pd.read_csv(\"Inputs/item_categories.csv\")\n",
    "item=pd.read_csv(\"Inputs/items.csv\")\n",
    "sub=pd.read_csv(\"Inputs/sample_submission.csv\")\n",
    "shops=pd.read_csv(\"Inputs/shops.csv\")\n",
    "test=pd.read_csv(\"Inputs/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa1c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2606: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['date', 'item_name', 'Trans_item', 'item_category_name', 'shop_name'], dtype='object')]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "sales=sales.join(item.set_index('item_id'),on='item_id',how='left')\n",
    "sales=sales.join(item_cat.set_index('item_category_id'),on='item_category_id',how='left')\n",
    "sales=sales.join(shops.set_index('shop_id'),on='shop_id',how='left')\n",
    "sales['Revenue']=sales['item_price']*sales['item_cnt_day']\n",
    "\n",
    "test=test.join(item.set_index('item_id'),on='item_id',how='left')\n",
    "test=test.join(item_cat.set_index('item_category_id'),on='item_category_id',how='left')\n",
    "test=test.join(shops.set_index('shop_id'),on='shop_id',how='left')\n",
    "test['date_block_num']=34\n",
    "sales=sales.append(test)\n",
    "\n",
    "sales=sales.loc[sales['item_category_id'].isin(test['item_category_id'])]\n",
    "sales.to_hdf(\"Inputs/sales.hdf\",key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b19e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from google_trans_new import google_translator  \n",
    "translator = google_translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "        '''\n",
    "            Changes column types in the dataframe: \n",
    "                    \n",
    "                    `float64` type to `float32`\n",
    "                    `int64`   type to `int32`\n",
    "        '''\n",
    "        \n",
    "        # Select columns to downcast\n",
    "        float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "        int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "        \n",
    "        # Downcast\n",
    "        df[float_cols] = df[float_cols].astype(np.float32)\n",
    "        df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_trans=pd.read_csv(\"Inputs/Completed_item_translations.csv\")\n",
    "#item_trans.columns=['item_name','Trans_item'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_translations = {}\n",
    "unique_elements = item_cat['item_category_name'].unique()\n",
    "for element in unique_elements:\n",
    "    # add translation to the dictionary\n",
    "    cat_translations[element] = translator.translate(element)\n",
    "    \n",
    "item_cat['Trans_cat']=item_cat['item_category_name'].map(cat_translations)\n",
    "sales['Trans_cats']=sales['item_category_name'].map(cat_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caae11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_translations = {}\n",
    "unique_elements = shops['shop_name'].unique()\n",
    "for element in unique_elements:\n",
    "    # add translation to the dictionary\n",
    "    shop_translations[element] = translator.translate(element)\n",
    "\n",
    "shops['Trans_shops']=shops['shop_name'].map(shop_translations)\n",
    "sales['Trans_shops']=sales['shop_name'].map(shop_translations)\n",
    "sales['Broad_cat']=sales['Trans_cats'].str.split('-').str[0]\n",
    "\n",
    "shops['shop_name'] = shops['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\n",
    "shops['shop_type'] = shops['shop_name'].apply(lambda x: 'мтрц' if 'мтрц' in x else 'трц' if 'трц' in x else 'трк' if 'трк' in x else 'тц' if 'тц' in x else 'тк' if 'тк' in x else 'NO_DATA')\n",
    "shop_types=shops.set_index('shop_id')['shop_type'].to_dict()\n",
    "sales['shop_type']=sales['shop_id'].map(shop_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_translations = {}\n",
    "unique_elements = sales['item_name'].unique()\n",
    "for element in unique_elements:\n",
    "    # add translation to the dictionary\n",
    "    # Need to add a sleep comment, because too many calls to google translator in a short period of time leads to being blocked out\n",
    "    time.sleep(1.25)\n",
    "    if element not in item_translations:\n",
    "       item_translations[element] = translator.translate(element)\n",
    "\n",
    "pd.DataFrame.from_dict(item_translations,orient='index').to_csv(\"Completed_item_translations.csv\")\n",
    "item['Trans_item']=item['item_name'].map(item_translations)\n",
    "item['Broad_item']=item['Trans_item'].str.split(' ').str[0:2].str.join(',').str.replace(',',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2c7998",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c93eff6bde55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msales\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msales\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'item_name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Broad_item'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'item_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "sales=pd.merge(sales,item[['item_name','Broad_item']],how='left',on='item_name')\n",
    "\n",
    "sales = downcast_dtypes(sales)\n",
    "sales.to_hdf(\"Inputs/Translated_sales.hdf\",key='df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c517d",
   "metadata": {},
   "source": [
    "Before doing below subsets, make it a story by doing visualizations to build all of it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980dc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc[sales['shop_id']==0,'Trans_shops']='Yakutsk Ordzhonikidze, 56 '\n",
    "sales.loc[sales['Trans_shops']=='Yakutsk Ordzhonikidze, 56 ','shop_id']=57\n",
    "sales.loc[sales['shop_id']==1,'Trans_shops']='Yakutsk shopping center \"Central\" '\n",
    "sales.loc[sales['Trans_shops']=='Yakutsk shopping center \"Central\" ','shop_id']=58\n",
    "sales.loc[sales['shop_id']==11,'Trans_shops']='Zhukovsky st. Chkalova 39m? '\n",
    "sales.loc[sales['Trans_shops']=='Zhukovsky st. Chkalova 39m? ','shop_id']=10\n",
    "sales.loc[sales['shop_id']==40,'Trans_shops']='Rostnone TRK \"Megacentr Horizont\"'\n",
    "sales.loc[sales['Trans_shops']=='Rostnone TRK \"Megacentr Horizont\"','shop_id']=39\n",
    "\n",
    "del sales['shop_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd724e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc[sales['date_block_num']==34,'item_price']=0.01\n",
    "sales = sales[(sales['item_price'] > 0) & (sales['item_price'] < 51000)]\n",
    "sales.loc[sales['date_block_num']==34,'item_cnt_day']=0\n",
    "sales = sales[sales['item_cnt_day'] <= 1000]\n",
    "\n",
    "\"\"\"Show visualization proof for why below\"\"\"\n",
    "sales=sales[(sales['shop_id']!=9)&(sales['shop_id']!=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567adbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['city']=sales['Trans_shops'].str.split(' ').str[0]\n",
    "sales.loc[sales['city']=='Voronež','city']='Voronezh'\n",
    "sales.loc[sales['city']=='SPb','city']='SPB'\n",
    "sales['city']=np.where(sales['city'].str.contains('online'),'Internet',sales['city'])\n",
    "sales['city']=np.where(sales['city'].str.contains('Online'),'Internet',sales['city'])\n",
    "sales['city']=np.where(sales['city']=='Rostnone','Rostovnadon',sales['city'])\n",
    "\n",
    "city_info = pd.read_csv('Inputs/city_info.csv')\n",
    "city_info['Trans_city']=city_info['Trans_city'].str.replace(' ','')\n",
    "city_info=city_info.loc[city_info.Trans_city.isin(sales['city'].unique())]\n",
    "city_info['city_size'].fillna(city_info['city_size'].mean(),inplace=True)\n",
    "city_sizes=city_info.set_index('Trans_city')['city_size'].to_dict()\n",
    "sales['city_size']=sales['city'].map(city_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal=pd.read_csv(\"Inputs/calendar.csv\")\n",
    "cal['date_block_num']=(cal['year']-2013)*12+cal['month']-1\n",
    "cal['hdays']=cal['mdays']-cal['wdays']\n",
    "sales=pd.merge(sales,cal[['date_block_num','mdays','hdays']],how='left',on='date_block_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4932be8f",
   "metadata": {},
   "source": [
    "Mention the source for above data and below codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_block_cnt = sales[sales['date_block_num']<34].groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].sum()\n",
    "items_to_drop = item_block_cnt[item_block_cnt <= 0].index\n",
    "sales = sales[~sales.set_index(['date_block_num', 'shop_id', 'item_id']).index.isin(items_to_drop)]\n",
    "\n",
    "sales_by_item_id = sales[sales['date_block_num']<34].pivot_table(index=['item_id'],values=['item_cnt_day'], \n",
    "                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\n",
    "sales_by_item_id.columns = sales_by_item_id.columns.droplevel().map(str)\n",
    "sales_by_item_id = sales_by_item_id.reset_index(drop=True).rename_axis(None, axis=1)\n",
    "sales_by_item_id.columns.values[0] = 'item_id'\n",
    "\n",
    "outdated_items = sales_by_item_id[sales_by_item_id.loc[:,'27':].sum(axis=1)==0]\n",
    "print('Outdated items:', len(outdated_items))\n",
    "\n",
    "print('Outdated items in test set:', len(test[test['item_id'].isin(outdated_items['item_id'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdated_test_items=test[test['item_id'].isin(outdated_items['item_id'])]\n",
    "outdated_test_items['open']=0\n",
    "outdated_test_items.set_index('ID').to_csv(\"Outdated_test_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc34791",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.drop(['item_category_name','Trans_cats', 'Trans_shops','ID','item_name','Trans_item'],axis=1,inplace=True)\n",
    "\n",
    "sales = downcast_dtypes(sales)\n",
    "sales.to_hdf(\"Inputs/Translated_sales_preprocessed.hdf\",key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb796a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_mod(all_data,fit_cols):   \n",
    "    all_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    all_data[fit_cols]=all_data[fit_cols].replace(0,np.nan)\n",
    "    \n",
    "    for col in fit_cols:\n",
    "        if 'dateblock' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num'])[col].transform('max'),inplace=True)\n",
    "        elif 'itemgroup' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','item_name_group'])[col].transform('max'),inplace=True)\n",
    "        elif 'itemartist' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','artist_name_or_first_word'])[col].transform('max'),inplace=True)\n",
    "        elif 'supercat' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','supercategory_id'])[col].transform('max'),inplace=True)\n",
    "        elif 'shoptypecat' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','shop_type','item_category_id'])[col].transform('max'),inplace=True)\n",
    "        elif 'shoptype' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','shop_type'])[col].transform('max'),inplace=True)\n",
    "        elif 'shopbroadcat' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','shop_id','Broad_cat'])[col].transform('max'),inplace=True)\n",
    "        elif 'shopcat' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','shop_id','item_category_id'])[col].transform('max'),inplace=True)\n",
    "        elif 'citybroadcat' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','city','Broad_cat'])[col].transform('max'),inplace=True)\n",
    "        elif 'citycat' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','city','item_category_id'])[col].transform('max'),inplace=True)\n",
    "        elif 'city' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','city'])[col].transform('max'),inplace=True)    \n",
    "           \n",
    "        \n",
    "        elif 'shop' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','shop_id'])[col].transform('max'),inplace=True)\n",
    "        elif 'broadcat' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','Broad_cat'])[col].transform('max'),inplace=True)\n",
    "        elif 'cat' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','item_category_id'])[col].transform('max'),inplace=True)\n",
    "        #elif 'all' in col:\n",
    "            #all_data[col].fillna(all_data.groupby(index_cols)[col].transform('max'),inplace=True)\n",
    "        elif 'broaditem' in col:\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','Broad_item'])[col].transform('max'),inplace=True) \n",
    "        elif 'item' in col and col!='Trans_item_lag_1':\n",
    "            all_data[col].fillna(all_data.groupby(['date_block_num','item_id'])[col].transform('max'),inplace=True)  \n",
    "    \n",
    "    all_data[fit_cols]=all_data[fit_cols].replace(np.nan,0)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ec779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53869e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "    \n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "grid = pd.merge(grid, sales.groupby(['item_id','Broad_cat','Broad_item','item_category_id'])['date'].count().reset_index().\\\n",
    "drop('date',axis=1), how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = sales.groupby(index_cols)['item_cnt_day'].agg(target='sum').reset_index()\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "gb = sales.groupby(index_cols)['item_cnt_day'].agg(target_count='count').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "del grid,gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_item_name(string):\n",
    "    # Removes bracketed terms, special characters and extra whitespace\n",
    "    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "    string = re.sub(r\"[^A-ZА-Яa-zа-я0-9 ]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "sim_thresh=65\n",
    "def partialmatchgroups(items, sim_thresh=sim_thresh):\n",
    "    def strip_brackets(string):\n",
    "        string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "        string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "        return string\n",
    "\n",
    "    items = items.copy()\n",
    "    items[\"nc\"] = items.item_name.apply(strip_brackets)\n",
    "    items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n",
    "\n",
    "    def partialcompare(s):\n",
    "        return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n",
    "\n",
    "    items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n",
    "    # Assign groups\n",
    "    grp = 0\n",
    "    for i in range(items.shape[0]):\n",
    "        items.loc[i, \"partialmatchgroup\"] = grp\n",
    "        if items.loc[i, \"partialmatch\"] < sim_thresh:\n",
    "            grp += 1\n",
    "    items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n",
    "    return items\n",
    "\n",
    "def extract_artist(st):\n",
    "    st = st.strip()\n",
    "    if st.startswith(\"V/A\"):\n",
    "        artist = \"V/A\"\n",
    "    elif st.startswith(\"СБ\"):\n",
    "        artist = \"СБ\"\n",
    "    else:\n",
    "        # Retrieves artist names using the double space or all uppercase pattern\n",
    "        mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n",
    "        match_dubspace = mus_artist_dubspace.match(st)\n",
    "        mus_artist_capsonly = re.compile(r\"^([^a-zа-я]+\\s)+\")\n",
    "        match_capsonly = mus_artist_capsonly.match(st)\n",
    "        candidates = [match_dubspace, match_capsonly]\n",
    "        candidates = [m[0] for m in candidates if m is not None]\n",
    "        # Sometimes one of the patterns catches some extra words so choose the shortest one\n",
    "        if len(candidates):\n",
    "            artist = min(candidates, key=len)\n",
    "        else:\n",
    "            # If neither of the previous patterns found something, use the dot-space pattern\n",
    "            mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n",
    "            match = mus_artist_dotspace.match(st)\n",
    "            if match:\n",
    "                artist = match[0]\n",
    "            else:\n",
    "                artist = \"\"\n",
    "    artist = artist.upper()\n",
    "    artist = re.sub(r\"[^A-ZА-Я ]||\\bTHE\\b\", \"\", artist)\n",
    "    artist = re.sub(r\"\\s{2,}\", \" \", artist)\n",
    "    artist = artist.strip()\n",
    "    return artist\n",
    "\n",
    "def first_word(string):\n",
    "    # This cleans the string of special characters, excess spaces and stopwords then extracts the first word\n",
    "    string = re.sub(r\"[^\\w\\s]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    tokens = string.lower().split()\n",
    "    tokens = [t for t in tokens if t not in all_stopwords]\n",
    "    token = tokens[0] if len(tokens) > 0 else \"\"\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c5689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_subset = items[['item_id', 'item_name','item_category_id']]\n",
    "items_subset[\"item_name_length\"] = items_subset[\"item_name\"].apply(len)\n",
    "items_subset[\"item_name_cleaned_length\"] = items_subset[\"item_name\"].apply(clean_item_name).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_subset = partialmatchgroups(items_subset)\n",
    "items_subset = items_subset.rename(columns={\"partialmatchgroup\": \"item_name_group\"})\n",
    "items_subset = items_subset.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n",
    "\n",
    "items_subset[\"item_name_group\"] = items_subset[\"item_name_group\"].apply(str)\n",
    "items_subset[\"item_name_group\"] = items_subset[\"item_name_group\"].factorize()[0]\n",
    "\n",
    "items_subset = items_subset.copy()\n",
    "all_stopwords = stopwords.words(\"russian\")\n",
    "all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "\n",
    "music_categories = [55, 56, 57, 58, 59, 60]\n",
    "items_subset.loc[items_subset.item_category_id.isin(music_categories), \"artist_name_or_first_word\"] = items_subset.loc[\n",
    "    items_subset.item_category_id.isin(music_categories), \"item_name\"\n",
    "].apply(extract_artist)\n",
    "items_subset.loc[items_subset[\"artist_name_or_first_word\"] == \"\", \"artist_name_or_first_word\"] = \"other music\"\n",
    "items_subset.loc[~items_subset.item_category_id.isin(music_categories), \"artist_name_or_first_word\"] = items_subset.loc[\n",
    "    ~items_subset.item_category_id.isin(music_categories), \"item_name\"\n",
    "].apply(first_word)\n",
    "items_subset.loc[items_subset[\"artist_name_or_first_word\"] == \"\", \"artist_name_or_first_word\"] = \"other non-music\"\n",
    "items_subset[\"artist_name_or_first_word\"] = items_subset[\"artist_name_or_first_word\"].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84309482",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_subset.drop('item_name',axis=1,inplace=True)\n",
    "items_subset.drop('item_category_id',axis=1,inplace=True)\n",
    "all_data=pd.merge(all_data,items_subset[['item_id','item_name_group','artist_name_or_first_word']],how='left',\\\n",
    "                  on='item_id')\n",
    "sales=pd.merge(sales,items_subset[['item_id','item_name_group','artist_name_or_first_word']],how='left',\\\n",
    "                  on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "supercat_map = {\n",
    "    0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 1, 11: 1, 12: 1,\n",
    "    13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 3, 21: 3, 22: 3, 23: 3,\n",
    "    24: 3, 25: 0, 26: 2, 27: 3, 28: 3, 29: 3, 30: 3, 31: 3, 32: 2, 33: 2, 34: 2,\n",
    "    35: 2, 36: 2, 37: 4, 38: 4, 39: 4, 40: 4, 41: 4, 42: 5, 43: 5, 44: 5, 45: 5,\n",
    "    46: 5, 47: 5, 48: 5, 49: 5, 50: 5, 51: 5, 52: 5, 53: 5, 54: 5, 55: 6, 56: 6,\n",
    "    57: 6, 58: 6, 59: 6, 60: 6, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0,\n",
    "    68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 7, 74: 7, 75: 7, 76: 7, 77: 7, 78: 7,\n",
    "    79: 2, 80: 2, 81: 0, 82: 0, 83: 0\n",
    "}\n",
    "all_data['supercategory_id'] = all_data['item_category_id'].map(supercat_map)\n",
    "sales['supercategory_id'] = sales['item_category_id'].map(supercat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b36938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = sales.loc[sales['item_cnt_day'] > 0].groupby(index_cols).agg({'item_cnt_day': 'count'})\n",
    "df_tmp.reset_index(inplace=True)\n",
    "df_tmp = df_tmp.rename(columns={'item_cnt_day': 'item_rate_month'})\n",
    "all_data = pd.merge(all_data, df_tmp, on=index_cols, how='left')\n",
    "all_data['item_rate_month'].fillna(0,inplace=True)\n",
    "del df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172cdad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, sales.groupby(['shop_id','shop_type','city'])['date'].count().reset_index().\\\n",
    "drop('date',axis=1), how='left', on='shop_id')\n",
    "all_data = pd.merge(all_data, sales.groupby(['city','city_size'])['date'].count().reset_index().\\\n",
    "drop('date',axis=1), how='left', on='city')   \n",
    "all_data=pd.merge(all_data,sales.groupby(['date_block_num','mdays','hdays'])['date'].count().reset_index().\\\n",
    "drop('date',axis=1),how='left',on='date_block_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24477a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPUTING COUNTS AND FREQUENCIES    \n",
    "gb = sales.groupby(['date_block_num','shop_id'])['item_id'].agg(shop_count='count').reset_index()\n",
    "gb2 = sales.groupby('date_block_num')['item_id'].agg(total_shop_count='count').reset_index()\n",
    "gb=pd.merge(gb,gb2,how='left',on='date_block_num')\n",
    "gb['shop_count_freq']=gb['shop_count']/gb['total_shop_count']\n",
    "all_data = pd.merge(all_data, gb[['date_block_num','shop_id','shop_count','shop_count_freq']], how='left', on=['date_block_num','shop_id']).fillna(0)\n",
    "\n",
    "gbi = sales.groupby(['date_block_num','item_id'])['shop_id'].agg(item_count='count').reset_index()\n",
    "gbi2 = sales.groupby('date_block_num')['shop_id'].agg(total_item_count='count').reset_index()\n",
    "gbi=pd.merge(gbi,gbi2,how='left',on='date_block_num')\n",
    "gbi['item_count_freq']=gbi['item_count']/gbi['total_item_count']\n",
    "all_data = pd.merge(all_data, gbi[['date_block_num','item_id','item_count','item_count_freq']], how='left', on=['date_block_num','item_id']).fillna(0)\n",
    "\n",
    "gbb = sales.groupby(['date_block_num','Broad_cat'])['shop_id'].agg(broadcat_count='count').reset_index()\n",
    "gbb2 = sales.groupby('date_block_num')['shop_id'].agg(total_broadcat_count='count').reset_index()\n",
    "gbb=pd.merge(gbb,gbb2,how='left',on='date_block_num')\n",
    "gbb['broadcat_count_freq']=gbb['broadcat_count']/gbb['total_broadcat_count']\n",
    "all_data = pd.merge(all_data, gbb[['date_block_num','Broad_cat','broadcat_count','broadcat_count_freq']], how='left', on=['date_block_num','Broad_cat']).fillna(0)\n",
    "\n",
    "gbb = sales.groupby(['date_block_num','item_category_id'])['shop_id'].agg(cat_count='count').reset_index()\n",
    "gbb2 = sales.groupby('date_block_num')['shop_id'].agg(total_cat_count='count').reset_index()\n",
    "gbb=pd.merge(gbb,gbb2,how='left',on='date_block_num')\n",
    "gbb['cat_count_freq']=gbb['cat_count']/gbb['total_cat_count']\n",
    "all_data = pd.merge(all_data, gbb[['date_block_num','item_category_id','cat_count','cat_count_freq']], how='left', on=['date_block_num','item_category_id']).fillna(0)\n",
    "\n",
    "gbb = sales.groupby(['date_block_num','supercategory_id'])['shop_id'].agg(supercat_count='count').reset_index()\n",
    "gbb2 = sales.groupby('date_block_num')['shop_id'].agg(total_supercat_count='count').reset_index()\n",
    "gbb=pd.merge(gbb,gbb2,how='left',on='date_block_num')\n",
    "gbb['supercat_count_freq']=gbb['supercat_count']/gbb['total_supercat_count']\n",
    "all_data = pd.merge(all_data, gbb[['date_block_num','supercategory_id','supercat_count','supercat_count_freq']], how='left', on=['date_block_num','supercategory_id']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca439f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = sales.groupby(['shop_id', 'date_block_num'])['item_cnt_day'].agg(target_shop='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "#all_data = pd.merge(all_data, broadcat_mapping, how='left', on='item_category_id')\n",
    "\n",
    "gb = sales.groupby(['shop_type', 'date_block_num'])['item_cnt_day'].agg(target_shoptype='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_type', 'date_block_num']).fillna(0)\n",
    "\n",
    "gb = sales.groupby(['city', 'date_block_num'])['item_cnt_day'].agg(target_city='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['city', 'date_block_num']).fillna(0)\n",
    "\n",
    "gb = sales.groupby(['item_id', 'date_block_num'])['item_cnt_day'].agg(target_item='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item category-month aggregates\n",
    "gb = sales.groupby(['item_category_id', 'date_block_num'])['item_cnt_day'].agg(target_itemcat='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_category_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "gb = sales.groupby(['Broad_cat', 'date_block_num'])['item_cnt_day'].agg(target_broadcat='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['Broad_cat', 'date_block_num']).fillna(0)\n",
    "\n",
    "gb = sales.groupby(['shop_id','item_category_id', 'date_block_num'])['item_cnt_day'].agg(target_shopcat='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id','item_category_id', 'date_block_num']).fillna(0)\n",
    "gb = sales.groupby(['shop_id','Broad_cat', 'date_block_num'])['item_cnt_day'].agg(target_shopbroadcat='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id','Broad_cat', 'date_block_num']).fillna(0)\n",
    "\n",
    "gb = sales.groupby(['city','item_category_id', 'date_block_num'])['item_cnt_day'].agg(target_citycat='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['city','item_category_id', 'date_block_num']).fillna(0)\n",
    "gb = sales.groupby(['city','Broad_cat', 'date_block_num'])['item_cnt_day'].agg(target_citybroadcat='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['city','Broad_cat', 'date_block_num']).fillna(0)\n",
    "\n",
    "gb = sales.groupby(['item_name_group', 'date_block_num'])['item_cnt_day'].agg(target_itemgroup='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_name_group', 'date_block_num']).fillna(0)\n",
    "\n",
    "gb = sales.groupby(['artist_name_or_first_word', 'date_block_num'])['item_cnt_day'].agg(target_itemartist='sum').reset_index()\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['artist_name_or_first_word', 'date_block_num']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics={'item_cnt_day':['mean'],'item_price':['mean'],'Revenue':['sum','mean']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474eb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "\n",
    "    # Groupby data to get shop-item-month aggregates\n",
    "    \n",
    "    if 'mean' in metrics[metric] and metric=='item_price':\n",
    "        gb = sales.groupby(index_cols,as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_allmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "        \n",
    "        # Same as above but with shop-month aggregates\n",
    "        gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_shopmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        \n",
    "        gb = sales.groupby(['city', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_citymean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['city', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['shop_type', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_shoptypemean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_type', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item-month aggregates\n",
    "        gb = sales.groupby(['item_id', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_itemmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['Broad_item', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_broaditemmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['Broad_item', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item category-month aggregates\n",
    "        gb = sales.groupby(['item_category_id', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_catmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_category_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['item_name_group', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_itemgroupmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_name_group', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['artist_name_or_first_word', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_itemartistmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['artist_name_or_first_word', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item category-month aggregates\n",
    "        gb = sales.groupby(['Broad_cat', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_broadcatmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['Broad_cat', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['shop_id','item_category_id', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_shopcatmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id','item_category_id', 'date_block_num']).fillna(0)\n",
    "        gb = sales.groupby(['shop_id','Broad_cat', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_shopbroadcatmean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id','Broad_cat', 'date_block_num']).fillna(0)\n",
    "        \n",
    "    elif 'mean' in metrics[metric]:\n",
    "        \n",
    "        #gb = sales.groupby(index_cols,as_index=False)[metric].mean()\n",
    "        #gb.rename(columns={metric:metric+'_allmean'},inplace=True)\n",
    "        #all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "        \n",
    "        # Same as above but with shop-month aggregates\n",
    "        gbs = sales.groupby(index_cols,as_index=False)[metric].sum()\n",
    "        \n",
    "        gbs = pd.merge(gbs, sales.groupby(['item_id','Broad_cat','Broad_item','item_category_id',\\\n",
    "        'artist_name_or_first_word','item_name_group'])['date'].count().reset_index().\\\n",
    "        drop('date',axis=1), how='left', on='item_id')\n",
    "        gbs = pd.merge(gbs, sales.groupby(['shop_id','shop_type','city'])['date'].count().reset_index().\\\n",
    "        drop('date',axis=1), how='left', on='shop_id')  \n",
    "        \n",
    "        gb=gbs.groupby(['shop_id', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_shopmeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        \n",
    "        gb=gbs.groupby(['city', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_citymeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['city', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = gbs.groupby(['shop_type', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_shoptypemean'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_type', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item-month aggregates\n",
    "        gb=gbs.groupby(['item_id', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_itemmeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item category-month aggregates\n",
    "        gb=gbs.groupby(['item_category_id', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_catmeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_category_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb=gbs.groupby(['item_name_group', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_itemgroupmeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_name_group', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb=gbs.groupby(['artist_name_or_first_word', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_itemartistmeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['artist_name_or_first_word', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item category-month aggregates\n",
    "        gb=gbs.groupby(['Broad_cat', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_broadcatmeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['Broad_cat', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = gbs.groupby(['shop_id','item_category_id', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_shopcatmeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id','item_category_id', 'date_block_num']).fillna(0)\n",
    "        gb = gbs.groupby(['shop_id','Broad_cat', 'date_block_num'],as_index=False)[metric].mean()\n",
    "        gb.rename(columns={metric:metric+'_shopbroadcatmeanx'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id','Broad_cat', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        del gbs\n",
    "    \n",
    "    if 'sum' in metrics[metric]:\n",
    "        gb = sales.groupby(index_cols,as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_allsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "        \n",
    "        # Same as above but with shop-month aggregates\n",
    "        gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_shopsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        \n",
    "        gb = sales.groupby(['city', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_citysum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['city', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['shop_type', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_shoptypesum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_type', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item-month aggregates\n",
    "        gb = sales.groupby(['item_id', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_itemsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['Broad_item', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_broaditemsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['Broad_item', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item category-month aggregates\n",
    "        gb = sales.groupby(['item_category_id', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_catsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_category_id', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['item_name_group', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_itemgroupsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['item_name_group', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['artist_name_or_first_word', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_itemartistsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['artist_name_or_first_word', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        # Same as above but with item category-month aggregates\n",
    "        gb = sales.groupby(['Broad_cat', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_broadcatsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['Broad_cat', 'date_block_num']).fillna(0)\n",
    "        \n",
    "        gb = sales.groupby('date_block_num',as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_dateblocksum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on='date_block_num').fillna(0)\n",
    "        \n",
    "        gb = sales.groupby(['shop_id','item_category_id', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_shopcatsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id','item_category_id', 'date_block_num']).fillna(0)\n",
    "        gb = sales.groupby(['shop_id','Broad_cat', 'date_block_num'],as_index=False)[metric].sum()\n",
    "        gb.rename(columns={metric:metric+'_shopbroadcatsum'},inplace=True)\n",
    "        all_data = pd.merge(all_data, gb, how='left', on=['shop_id','Broad_cat', 'date_block_num']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['binned_item_price_itemmean']=pd.qcut(all_data['item_price_itemmean'],q=12)\n",
    "\n",
    "gb=all_data.groupby(['binned_item_price_itemmean', 'date_block_num'],as_index=False)['target'].count()\n",
    "gb=gb.rename(columns={'target':'freq_encoded_item_count_freq'})\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['binned_item_price_itemmean', 'date_block_num'])\n",
    "del all_data['binned_item_price_itemmean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    # Use of hybrid method\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_first_sale=pd.DataFrame(columns=['item_id','first_item_sale_month'])\n",
    "\n",
    "items_left=sales['item_id'].unique().tolist()\n",
    "\n",
    "for month in sales['date_block_num'].unique():            \n",
    "    out=intersection(items_left,sales.loc[sales['date_block_num']==month,'item_id'].unique().tolist())\n",
    "    month_sales=pd.DataFrame([out,[month]*len(out)]).T\n",
    "    month_sales.columns=['item_id','first_item_sale_month']\n",
    "    \n",
    "    items_first_sale=items_first_sale.append(month_sales)\n",
    "    items_left=list(set(items_left).symmetric_difference(set(out)))\n",
    "\n",
    "all_data['first_item_sale_month']=all_data['item_id'].map(items_first_sale.set_index(\\\n",
    "'item_id')['first_item_sale_month'].to_dict())\n",
    "\n",
    "all_data['mths_since_item_first']=all_data['date_block_num']-all_data['first_item_sale_month']  \n",
    "all_data['mths_since_item_first']=np.where(all_data['mths_since_item_first']<-1,-1,all_data['mths_since_item_first'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a533365",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['unique_item_groups_month']=all_data.groupby(['date_block_num','item_name_group'])['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_item_artists_month']=all_data.groupby(['date_block_num',\"artist_name_or_first_word\"])['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_item_cats_month']=all_data.groupby(['date_block_num',\"item_category_id\"])['item_id'].transform(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['unique_item_groups_restric_month']=all_data.loc[all_data['mths_since_item_first']==0].groupby(['date_block_num','item_name_group'])['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_item_groups_restric_month'].fillna(0,inplace=True)\n",
    "all_data['unique_item_artists_restric_month']=all_data.loc[all_data['mths_since_item_first']==0].groupby(['date_block_num',\"artist_name_or_first_word\"])['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_item_artists_restric_month'].fillna(0,inplace=True)\n",
    "all_data['unique_item_cats_restric_month']=all_data.loc[all_data['mths_since_item_first']==0].groupby(['date_block_num',\"item_category_id\"])['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_item_cats_restric_month'].fillna(0,inplace=True)\n",
    "\n",
    "all_data.drop(['first_item_sale_month','mths_since_item_first'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf227a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['date']=pd.to_datetime(sales['date'], format=\"%d.%m.%Y\")\n",
    "month_last_day = sales.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n",
    "month_last_day[~month_last_day.dt.is_month_end] = (month_last_day[~month_last_day.dt.is_month_end] + MonthEnd())\n",
    "\n",
    "month_first_day = sales.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n",
    "month_first_day[~month_first_day.dt.is_month_start] = (month_first_day[~month_first_day.dt.is_month_start] - MonthBegin())\n",
    "\n",
    "month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n",
    "first_shop_date = sales.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n",
    "first_item_date = sales.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n",
    "#first_shop_item_date = (sales.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac917805",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38880320",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"shop_open_days\"] = all_data[\"month_last_day\"] - all_data[\"first_shop_date\"] + Day()\n",
    "all_data[\"item_first_sale_days\"] = all_data[\"month_last_day\"] - all_data[\"first_item_date\"] + Day()\n",
    "all_data[\"item_in_shop_days\"] = (all_data[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days)\n",
    "\n",
    "all_data[\"item_cnt_day_avg\"] = all_data[\"target\"] / all_data[\"item_in_shop_days\"]\n",
    "\n",
    "#all_data['target2']=all_data[\"item_cnt_day_avg\"]*all_data[\"month_length\"]\n",
    "all_data = all_data.drop(columns=[\"item_first_sale_days\",\"item_in_shop_days\",\"shop_open_days\",\\\n",
    "            \"month_last_day\",\"first_item_date\",\"month_length\",'month_first_day', 'first_shop_date'])\n",
    "\n",
    "del gb,sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001144e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['shop_item_item_cnt_day_avg']=all_data.groupby(['date_block_num','shop_id','item_id'])['item_cnt_day_avg'].transform('mean')\n",
    "\n",
    "all_data['shop_itemcat_item_cnt_day_avg']=all_data.groupby(['date_block_num','shop_id','item_category_id'])['item_cnt_day_avg'].transform('mean')\n",
    "all_data['shop_itemgroup_item_cnt_day_avg']=all_data.groupby(['date_block_num','shop_id','item_name_group'])['item_cnt_day_avg'].transform('mean')\n",
    "all_data['shop_supercat_item_cnt_day_avg']=all_data.groupby(['date_block_num','shop_id','supercategory_id'])['item_cnt_day_avg'].transform('mean')\n",
    "\n",
    "all_data['city_item_item_cnt_day_avg']=all_data.groupby(['date_block_num','city','item_id'])['item_cnt_day_avg'].transform('mean')\n",
    "all_data['city_item_artist_cnt_day_avg']=all_data.groupby(['date_block_num','city','artist_name_or_first_word'])['item_cnt_day_avg'].transform('mean')\n",
    "all_data['city_itemcat_item_cnt_day_avg']=all_data.groupby(['date_block_num','city','item_category_id'])['item_cnt_day_avg'].transform('mean')\n",
    "\n",
    "all_data['city_supercat_item_cnt_day_avg']=all_data.groupby(['date_block_num','city','supercategory_id'])['item_cnt_day_avg'].transform('mean')\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_rename = list(all_data.columns.difference(index_cols+['item_category_id','Broad_cat','Trans_item',\\\n",
    "'Broad_item','city','shop_type','hdays','mdays','city_size',\\\n",
    "'artist_name_or_first_word','item_name_group','supercategory_id']))\n",
    "#shift_range = [12,9,6,3,2,1]\n",
    "shift_range = [i for i in range(12,0,-1)]\n",
    "lags_to_keep=[1,2,3,6,9,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa531b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month_shift in [3,6,12]:\n",
    "    all_data[[i for i in range(len(cols_to_rename))]]=0.0\n",
    "    new_colnames=[]\n",
    "    for i in range(len(cols_to_rename)):\n",
    "        new_colnames.append(cols_to_rename[i]+'_roll_'+str(month_shift)+'_mean')\n",
    "    all_data.columns=all_data.columns[:len(all_data.columns)-len(cols_to_rename)].tolist()+new_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff=1\n",
    "for month_shift in shift_range:\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "    \n",
    "    #all_data=all_data[all_data['date_block_num']>=cutoff]\n",
    "    gc.collect()\n",
    "    train_shift=train_shift[train_shift['date_block_num']<=34]\n",
    "    gc.collect()\n",
    "    \n",
    "    for_merge=all_data[index_cols+['item_category_id','Broad_cat',\\\n",
    "    'Broad_item','city','shop_type','artist_name_or_first_word','item_name_group','supercategory_id']]\n",
    "    for_merge = pd.merge(for_merge, train_shift, on=index_cols, how='left').fillna(0) \n",
    "    \n",
    "    del train_shift\n",
    "    gc.collect()\n",
    "    \n",
    "    for_merge=fillna_mod(for_merge,for_merge.columns[11:])\n",
    "    gc.collect()\n",
    "    \n",
    "    if month_shift in lags_to_keep:\n",
    "        all_data[for_merge.columns[11:]]=for_merge[for_merge.columns[11:]].values\n",
    "    \n",
    "    for month in [3,6,12]:\n",
    "        new_colnames=[]\n",
    "        for i in range(len(cols_to_rename)):\n",
    "            new_colnames.append(cols_to_rename[i]+'_roll_'+str(month)+'_mean')\n",
    "        i=0\n",
    "        for col in new_colnames:\n",
    "            all_data[col]=all_data[col].add(for_merge[for_merge.columns[11+i]].values)\n",
    "            i+=1\n",
    "        #all_data[new_colnames]=all_data[new_colnames].add(for_merge[for_merge.columns[11:]].values)\n",
    "        gc.collect()\n",
    "    \n",
    "    #all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0) \n",
    "\n",
    "    print(str(month_shift)+\" lags complete\")\n",
    "    #del train_shift\n",
    "    del for_merge\n",
    "    gc.collect()\n",
    "    cutoff+=1\n",
    "    #cutoff=min(cutoff,9)    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in [3,6,12]:\n",
    "    new_colnames=[]\n",
    "    for i in range(len(cols_to_rename)):\n",
    "        new_colnames.append(cols_to_rename[i]+'_roll_'+str(month)+'_mean')\n",
    "    for col in new_colnames:\n",
    "        all_data[col]=all_data[col]/month\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b148c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.loc[all_data['date_block_num']>=18]\n",
    "all_data.to_pickle(\"Inputs/Base_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "shift_range = [12,9,6,3,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['target_ratio_1_2']=all_data['target_lag_1']/all_data['target_lag_2']\n",
    "all_data['target_ratio_2_3']=all_data['target_lag_2']/all_data['target_lag_3']\n",
    "rat_cols=['target_ratio_1_2','target_ratio_2_3']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff96eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols=[]\n",
    "for col in all_data.columns:\n",
    "    if col.endswith('roll_3_mean') or col.endswith('roll_6_mean') or col.endswith('roll_12_mean'):\n",
    "        mean_cols.append(col)\n",
    "        \n",
    "uniq_cols=[]\n",
    "for col in all_data.columns:\n",
    "    if 'unique' in col:\n",
    "        uniq_cols.append(col)\n",
    "\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range] or col[-2:] in [str(item) for item in shift_range]]\n",
    "#fit_cols+=rat_cols\n",
    "fit_cols+=mean_cols\n",
    "fit_cols+=uniq_cols\n",
    "\n",
    "to_drop_cols = list(set(list(all_data.columns))-(set(fit_cols)|set(['shop_id', 'date_block_num','item_category_id',\\\n",
    "'city','city_size','shop_type','hdays','mdays','Broad_cat','target','artist_name_or_first_word','item_name_group',\\\n",
    "'item_id','supercategory_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de01489",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[rat_cols].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#all_data[rat_cols]=all_data[rat_cols].replace(np.nan,0)\n",
    "for col in rat_cols:\n",
    "    all_data[col].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe718ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_subset = items[['item_id', 'item_name','item_category_id']]\n",
    "\n",
    "feature_count = 25\n",
    "tfidf = TfidfVectorizer(max_features=feature_count)\n",
    "items_text_fts = pd.DataFrame(tfidf.fit_transform(items_subset['item_name']).toarray())\n",
    "\n",
    "cols =items_text_fts.columns\n",
    "for i in range(feature_count):\n",
    "    feature_name = 'item_name_tfidf_' + str(i)\n",
    "    items_subset[feature_name] = items_text_fts[cols[i]]\n",
    "    \n",
    "items_subset.drop('item_name',axis=1,inplace=True)\n",
    "items_subset.drop('item_category_id',axis=1,inplace=True)\n",
    "\n",
    "all_data=all_data.merge(items_subset,how='left',on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89202878",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_map = {\n",
    "    0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 8, 10: 1, 11: 2,\n",
    "    12: 3, 13: 4, 14: 5, 15: 6, 16: 7, 17: 8, 18: 1, 19: 2, 20: 3, 21: 4, 22: 5,\n",
    "    23: 6, 24: 7, 25: 8, 26: 9, 27: 10, 28: 0, 29: 0, 30: 0, 31: 0, 32: 8, 33: 11,\n",
    "    34: 11, 35: 3, 36: 0, 37: 12, 38: 12, 39: 12, 40: 13, 41: 13, 42: 14, 43: 15,\n",
    "    44: 15, 45: 15, 46: 14, 47: 14, 48: 14, 49: 14, 50: 14, 51: 14, 52: 14, 53: 14,\n",
    "    54: 8, 55: 16, 56: 16, 57: 17, 58: 18, 59: 13, 60: 16, 61: 8, 62: 8, 63: 8, 64: 8,\n",
    "    65: 8, 66: 8, 67: 8, 68: 8, 69: 8, 70: 8, 71: 8, 72: 8, 73: 0, 74: 10, 75: 0,\n",
    "    76: 0, 77: 0, 78: 0, 79: 8, 80: 8, 81: 8, 82: 8, 83: 8,\n",
    "}\n",
    "all_data['platform_id'] = all_data['item_category_id'].map(platform_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    # Use of hybrid method\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3\n",
    "\n",
    "sales=pd.read_hdf(\"Inputs/Translated_sales_preprocessed.hdf\")\n",
    "\n",
    "sales['shop_item']=sales['shop_id'].astype('str')+'_'+sales['item_id'].astype('str')\n",
    "sales['shop_itemcat']=sales['shop_id'].astype('str')+'_'+sales['item_category_id'].astype('str')\n",
    "\n",
    "items_first_sale=pd.DataFrame(columns=['item_id','first_item_sale_month'])\n",
    "shops_first_sale=pd.DataFrame(columns=['shop_id','first_shop_sale_month'])\n",
    "shopitems_first_sale=pd.DataFrame(columns=['shop_item','first_shopitem_sale_month'])\n",
    "shopitemcats_first_sale=pd.DataFrame(columns=['shop_itemcat','first_shopitemcat_sale_month'])\n",
    "\n",
    "items_left=sales['item_id'].unique().tolist()\n",
    "shops_left=sales['shop_id'].unique().tolist()\n",
    "shopitems_left=sales['shop_item'].unique().tolist()\n",
    "shopitemcats_left=sales['shop_itemcat'].unique().tolist()\n",
    "\n",
    "for month in sales['date_block_num'].unique():            \n",
    "    out=intersection(items_left,sales.loc[sales['date_block_num']==month,'item_id'].unique().tolist())\n",
    "    month_sales=pd.DataFrame([out,[month]*len(out)]).T\n",
    "    month_sales.columns=['item_id','first_item_sale_month']\n",
    "    \n",
    "    items_first_sale=items_first_sale.append(month_sales)\n",
    "    items_left=list(set(items_left).symmetric_difference(set(out)))\n",
    "    \n",
    "    \n",
    "    out=intersection(shops_left,sales.loc[sales['date_block_num']==month,'shop_id'].unique().tolist())\n",
    "    month_sales=pd.DataFrame([out,[month]*len(out)]).T\n",
    "    month_sales.columns=['shop_id','first_shop_sale_month']\n",
    "    \n",
    "    shops_first_sale=shops_first_sale.append(month_sales)\n",
    "    shops_left=list(set(shops_left).symmetric_difference(set(out)))\n",
    "    \n",
    "    \n",
    "    out=intersection(shopitems_left,sales.loc[sales['date_block_num']==month,'shop_item'].unique().tolist())\n",
    "    month_sales=pd.DataFrame([out,[month]*len(out)]).T\n",
    "    month_sales.columns=['shop_item','first_shopitem_sale_month']\n",
    "    \n",
    "    shopitems_first_sale=shopitems_first_sale.append(month_sales)\n",
    "    shopitems_left=list(set(shopitems_left).symmetric_difference(set(out)))\n",
    "    \n",
    "    out=intersection(shopitemcats_left,sales.loc[sales['date_block_num']==month,'shop_itemcat'].unique().tolist())\n",
    "    month_sales=pd.DataFrame([out,[month]*len(out)]).T\n",
    "    month_sales.columns=['shop_itemcat','first_shopitemcat_sale_month']\n",
    "    \n",
    "    shopitemcats_first_sale=shopitemcats_first_sale.append(month_sales)\n",
    "    shopitemcats_left=list(set(shopitemcats_left).symmetric_difference(set(out)))\n",
    "    \n",
    "#shopitems_first_sale.groupby('first_shopitem_sale_month')['shop_item'].count()\n",
    "#shopitemcats_first_sale.groupby('first_shopitemcat_sale_month')['shop_itemcat'].count()\n",
    "\n",
    "del sales\n",
    "gc.collect()\n",
    "\n",
    "all_data['first_item_sale_month']=all_data['item_id'].map(items_first_sale.set_index(\\\n",
    "'item_id')['first_item_sale_month'].to_dict())\n",
    "all_data['first_shop_sale_month']=all_data['shop_id'].map(shops_first_sale.set_index(\\\n",
    "'shop_id')['first_shop_sale_month'].to_dict())\n",
    "\n",
    "all_data['shop_item']=all_data['shop_id'].astype('str')+'_'+all_data['item_id'].astype('str')\n",
    "all_data['shop_itemcat']=all_data['shop_id'].astype('str')+'_'+all_data['item_category_id'].astype('str')\n",
    "\n",
    "all_data['first_shopitemcat_sale_month']=all_data['shop_itemcat'].map(shopitemcats_first_sale.set_index(\\\n",
    "'shop_itemcat')['first_shopitemcat_sale_month'].to_dict())\n",
    "all_data['first_shopitem_sale_month']=all_data['shop_item'].map(shopitems_first_sale.set_index(\\\n",
    "'shop_item')['first_shopitem_sale_month'].to_dict())\n",
    "\n",
    "all_data['mths_since_shopitem_first']=all_data['date_block_num']-all_data['first_shopitem_sale_month']   \n",
    "all_data['mths_since_shopitem_first']=np.where(all_data['mths_since_shopitem_first']<-1,-1,all_data['mths_since_shopitem_first'])\n",
    "all_data['mths_since_shopitemcat_first']=all_data['date_block_num']-all_data['first_shopitemcat_sale_month']   \n",
    "all_data['mths_since_shopitemcat_first']=np.where(all_data['mths_since_shopitemcat_first']<-1,-1,all_data['mths_since_shopitemcat_first'])\n",
    "all_data['mths_since_item_first']=all_data['date_block_num']-all_data['first_item_sale_month']   \n",
    "all_data['mths_since_item_first']=np.where(all_data['mths_since_item_first']<-1,-1,all_data['mths_since_item_first'])\n",
    "all_data['mths_since_shop_first']=all_data['date_block_num']-all_data['first_shop_sale_month']   \n",
    "all_data['mths_since_shop_first']=np.where(all_data['mths_since_shop_first']<-1,-1,all_data['mths_since_shop_first'])\n",
    "\n",
    "#del all_data['mths_since_shop_first']\n",
    "\n",
    "all_data['mths_since_shopitemcat_first'].fillna(0,inplace=True)\n",
    "\n",
    "\"\"\"Adding some unique items features to discover data leakages\"\"\"\n",
    "all_data['unique_items_month']=all_data.groupby('date_block_num')['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_item_group_cats_month']=all_data.groupby(['date_block_num','item_name_group','item_category_id'])['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_item_artist_cats_month']=all_data.groupby(['date_block_num',\"artist_name_or_first_word\",'item_category_id'])['item_id'].transform(lambda x: x.nunique())\n",
    "\n",
    "all_data['unique_items_restric_month']=all_data.loc[all_data['mths_since_item_first']==0].groupby('date_block_num')['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_items_restric_month'].fillna(0,inplace=True)\n",
    "all_data['unique_item_group_cats_restric_month']=all_data.loc[all_data['mths_since_item_first']==0].groupby(['date_block_num','item_name_group','item_category_id'])['item_id'].transform(lambda x: x.nunique())\n",
    "all_data['unique_item_group_cats_restric_month'].fillna(0,inplace=True)\n",
    "\n",
    "all_data['mths_since_shopitemcat_first'].fillna(0,inplace=True)\n",
    "\n",
    "#all_data.drop('first_shop_sale_month',axis=1,inplace=True)\n",
    "all_data.drop(['shop_item','shop_itemcat','first_shopitem_sale_month','first_shopitemcat_sale_month','first_shop_sale_month',\\\n",
    "'first_item_sale_month','artist_name_or_first_word','item_name_group'],axis=1,inplace=True)\n",
    "\n",
    "all_data['mths_since_item_first']=all_data['mths_since_item_first'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f780475",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['seasonal']=12-all_data['date_block_num']%12\n",
    "all_data=all_data.drop(to_drop_cols,axis=1)\n",
    "\n",
    "del all_data['shop_type']\n",
    "del all_data['mths_since_shopitem_first']\n",
    "\n",
    "all_data = downcast_dtypes(all_data)\n",
    "\n",
    "all_data['city']=all_data['city'].astype('category')\n",
    "all_data['city']=all_data['city'].cat.codes\n",
    "\n",
    "#all_data['shop_type']=all_data['shop_type'].astype('category')\n",
    "#all_data['shop_type']=all_data['shop_type'].cat.codes\n",
    "\n",
    "all_data['Broad_cat']=all_data['Broad_cat'].astype('category')\n",
    "all_data['Broad_cat']=all_data['Broad_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['date']=pd.to_datetime(sales['date'], format=\"%d.%m.%Y\")\n",
    "month_last_day = sales.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n",
    "month_last_day[~month_last_day.dt.is_month_end] = (month_last_day[~month_last_day.dt.is_month_end] + MonthEnd())\n",
    "\n",
    "month_first_day = sales.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n",
    "month_first_day[~month_first_day.dt.is_month_start] = (month_first_day[~month_first_day.dt.is_month_start] - MonthBegin())\n",
    "\n",
    "month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n",
    "first_shop_date = sales.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n",
    "first_item_date = sales.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n",
    "first_shop_item_date = (sales.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\"))\n",
    "\n",
    "all_data = all_data.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "all_data = all_data.merge(first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\")\n",
    "\n",
    "all_data[\"shop_open_days\"] = all_data[\"month_last_day\"] - all_data[\"first_shop_date\"] + Day()\n",
    "all_data[\"item_first_sale_days\"] = all_data[\"month_last_day\"] - all_data[\"first_item_date\"] + Day()\n",
    "all_data[\"item_in_shop_days\"] = (all_data[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days)\n",
    "\n",
    "all_data = all_data.drop(columns=\"item_first_sale_days\")\n",
    "all_data[\"item_cnt_day_avg\"] = all_data[\"target\"] / all_data[\"item_in_shop_days\"]\n",
    "all_data[\"month_length\"] = all_data[\"month_length\"].dt.days\n",
    "\n",
    "all_data[\"shop_open_days\"] = all_data[\"month_first_day\"] - all_data[\"first_shop_date\"]\n",
    "all_data[\"first_item_sale_days\"] = all_data[\"month_first_day\"] - all_data[\"first_item_date\"]\n",
    "all_data[\"first_shop_item_sale_days\"] = all_data[\"month_first_day\"] - all_data[\"first_shop_item_date\"]\n",
    "#m[\"first_name_group_sale_days\"] = m[\"month_first_day\"] - m[\"first_name_group_date\"]\n",
    "all_data[\"shop_open_days\"] = all_data[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n",
    "all_data[\"first_item_sale_days\"] = (all_data[\"first_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999))\n",
    "all_data[\"first_shop_item_sale_days\"] = (all_data[\"first_shop_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d613b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_sale_days(m):\n",
    "    last_shop_item_dates = []\n",
    "    for dbn in range(1, 35):\n",
    "        lsid_temp = (sales.query(f\"date_block_num<{dbn}\").groupby([\"shop_id\", \"item_id\"]).date.max()\n",
    "        .rename(\"last_shop_item_sale_date\").reset_index())\n",
    "        lsid_temp[\"date_block_num\"] = dbn\n",
    "        last_shop_item_dates.append(lsid_temp)\n",
    "\n",
    "    last_shop_item_dates = pd.concat(last_shop_item_dates)\n",
    "    m = m.merge(last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\")\n",
    "\n",
    "    def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n",
    "        m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n",
    "        m.loc[m[feat_name] > 2000, feat_name] = missingval\n",
    "        m.loc[m[feat_name].isna(), feat_name] = missingval\n",
    "        return m\n",
    "\n",
    "    m = days_since_last_feat(m, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999)\n",
    "\n",
    "    m = m.drop(columns=[\"last_shop_item_sale_date\"])\n",
    "    return m\n",
    "\n",
    "all_data = last_sale_days(all_data)\n",
    "# Month id feature\n",
    "all_data[\"month\"] = all_data[\"month_first_day\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c64dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop(columns=[\"first_day\",\"month_first_day\",\"month_last_day\",\"first_shop_date\",\"first_item_date\"\\\n",
    "                    ,\"month\",\"item_in_shop_days\",\"first_shop_item_date\",\"month_length\"],errors=\"ignore\")\n",
    "\n",
    "all_data[\"cat_items_proportion\"] = all_data[\"unique_item_cats_month\"] / all_data[\"unique_items_month\"]\n",
    "all_data[\"name_group_new_proportion_month\"] = (all_data['unique_item_groups_restric_month'] / all_data['unique_items_restric_month'])\n",
    "all_data = all_data.drop(columns=[\"item_cnt_day_avg\",\"unique_items_month\", 'unique_items_restric_month'])\n",
    "#Change version according to monthly aggregation method used\n",
    "all_data.to_pickle(\"Inputs/Base_train2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29050137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.sparse \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import gc\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from statsmodels.regression import linear_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "out_test_items=pd.read_csv(\"Outdated_test_items.csv\")\n",
    "sub=pd.read_csv(\"Inputs/sample_submission.csv\")\n",
    "test=pd.read_csv(\"Inputs/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d4fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speed='slow'\n",
    "\n",
    "if train_speed=='fast':\n",
    "    lrate=0.04\n",
    "    niter=100\n",
    "elif train_speed=='slow':\n",
    "    lrate=0.004\n",
    "    niter=1000\n",
    "else:\n",
    "    lrate=0.0004\n",
    "    niter=5000\n",
    "    \n",
    "last_block=34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26911b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['item_id','shop_id'],axis=1,inplace=True)\n",
    "\n",
    "#del all_data['item_id']\n",
    "X_train = all_data.loc[all_data['date_block_num'] <  last_block].drop('target', axis=1)\n",
    "X_test =  all_data.loc[all_data['date_block_num'] == last_block].drop('target', axis=1)\n",
    "\n",
    "y_train = all_data.loc[all_data['date_block_num'] <  last_block, 'target'].clip(0,20).values\n",
    "y_test =  all_data.loc[all_data['date_block_num'] == last_block, 'target'].clip(0,20).values\n",
    "\n",
    "del all_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':8, \n",
    "               'min_data_in_leaf': 20, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': lrate, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**9,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0,\n",
    "               'num_iterations':niter,\n",
    "               'categorical_feature':['name:item_category_id','name:city',\\\n",
    "                                      'name:Broad_cat','name:platform_id','name:supercategory_id','name:seasonal']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train,categorical_feature=['item_category_id',\\\n",
    "'Broad_cat','city','supercategory_id','platform_id','seasonal']), niter)\n",
    "    \n",
    "pred_lgb = model.predict(X_test)\n",
    "pred_train=model.predict(X_train)\n",
    "#pred_lgb=np.exp(pred_lgb)-0.01-make_pos\n",
    "\n",
    "pred_lgb=pred_lgb.clip(0,20)\n",
    "pred_train=pred_train.clip(0,20)\n",
    "\n",
    "print('R-squared for TRAIN - %f' % r2_score(y_train, pred_train))\n",
    "print('RMSE for LightGBM - %f' % mse(y_train, pred_train,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04445966",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['item_cnt_month']=pred_lgb\n",
    "sub['item_cnt_month']=sub['item_cnt_month'].clip(0,20)\n",
    "lgb.plot_importance(model,max_num_features=20)\n",
    "lgb.plot_importance(model,max_num_features=20,importance_type='gain')\n",
    "\n",
    "#sub['item_cnt_month'].mean()\n",
    "\n",
    "#simple_sub=pd.read_csv(\"Subs/slow_6rollmean_bomb_simple_sub_bigdata_clipped.csv\")\n",
    "sub=sub.merge(test,on='ID')\n",
    "\n",
    "sub=pd.merge(sub,out_test_items,how='left',on=['ID','shop_id','item_id'])\n",
    "\n",
    "sub2=sub.copy()\n",
    "sub2['item_cnt_month']=np.where(sub2['open']==sub2['open'],0,sub2['item_cnt_month'])\n",
    "\n",
    "sub2['item_cnt_month'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2.set_index('ID')['item_cnt_month'].to_csv(\"Subs/slow_moretime_tfidf_rollmeaned_nobc_simple_sub_smalldata_clipped.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
